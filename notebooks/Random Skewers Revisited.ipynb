{
 "metadata": {
  "name": "",
  "signature": "sha256:ce8c860f1a779cb9872e45760c7c2489ccd798fa79011466a2386f7e0abbbb13"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Considerando a decomposi\u00e7\u00e3o espectral das matrizes $A$ e $B$, com autovetores $u_i$ e $s_i$ formando bases ortonormais:\n",
      "\n",
      "$ A = \\sum_{i} \\lambda_i u_i u_i^{T} $ e $ B = \\sum_{i} \\gamma_i s_i s_i^{T} $ \n",
      "\n",
      "Pelo m\u00e9todo Random Skewers cria-se vetores aleat\u00f3rios unit\u00e1rios $\\beta$ e calcula-se a m\u00e9dia:\n",
      "\n",
      "$ < \\overline{A \\beta} \\cdot \\overline{B \\beta} > $\n",
      "\n",
      "Onde $ \\overline{A \\beta} $ \u00e9 igual ao valor $A \\beta$ normalizado ($ |A\\beta|= 1$). Este valor \u00e9 utilizado como uma m\u00e9trica de qu\u00e3o diferente s\u00e3o as matrizes $A$ e $B$ em rela\u00e7\u00e3o \u00e0 respostas para diferentes vetores de sele\u00e7\u00e3o.\n",
      "\n",
      "Utilizando que $A \\cdot B = A^TB$ e $ (AB)^T =  B^T A^T$:\n",
      "\n",
      "$ \\overline{A \\beta} \\cdot \\overline{B \\beta} = \\frac{1}{N_A N_B}(\\sum_{i} \\lambda_i u_i u_i^{T}\\beta)^T(\\sum_{j} \\gamma_j s_j s_j^{T}\\beta) $\n",
      "\n",
      "Onde $N_A$ e $N_B$ s\u00e3o os termos utilizados para a normaliza\u00e7\u00e3o, continuando:\n",
      "\n",
      "$\\frac{1}{N_A N_B} (\\sum_{i} \\beta^T\\lambda_i u_i u_i^{T})(\\sum_{j} \\gamma_j s_j s_j^{T}\\beta) = \\frac{1}{N_A N_B} (\\sum_{i,j} \\lambda_i \\gamma_j \\beta^T u_i u_i^{T} s_j s_j^{T}\\beta) $ \n",
      "\n",
      "Com $u_i^Ts_j = u_i \\cdot s_j = cos\\theta_{ij}$\n",
      "\n",
      "$ \\beta^T u_i = u_i \\cdot \\beta = cos\\theta_i^{A}$\n",
      "\n",
      "$ s_j^T \\beta = s_j \\cdot \\beta = cos\\theta_j^{B}$\n",
      "\n",
      "Teremos\n",
      "\n",
      "$ \\frac{1}{N_A N_B} (\\sum_{i,j} \\lambda_i \\gamma_j cos\\theta_{ij} cos\\theta_i^{A} cos\\theta_j^{B}) $ \n",
      "\n",
      "Para acharmos $N_A$ e $N_B$ basta considera a express\u00e3o:\n",
      "\n",
      "$  A \\beta \\cdot A \\beta  = |A\\beta|^2$  \n",
      "\n",
      "$ A \\beta \\cdot A \\beta = \\sum_{i,j} \\lambda_i \\lambda_j cos\\theta_{ij} cos\\theta_i^{A} cos\\theta_j^{A} $\n",
      "\n",
      "E como $cos\\theta_{ij} = 0$ para $i \\neq j$ e $1$ caso contr\u00e1rio\n",
      "\n",
      "$ A \\beta \\cdot A \\beta = \\sum_{i} \\lambda_i^2 cos^2\\theta_i^{A} $\n",
      "\n",
      "Logo\n",
      "\n",
      "$N_A = \\frac{1}{\\sqrt{\\sum_{i} \\lambda_i^2 cos^2\\theta_i^{A}}} $\n",
      "\n",
      "e equivalentemente:\n",
      "\n",
      "$N_B = \\frac{1}{\\sqrt{\\sum_{i} \\gamma_i^2 cos^2\\theta_i^{B}}} $\n",
      "\n",
      "$ RS(A,B) = \\langle \\frac{\\sum_{i,j} \\lambda_i \\gamma_j cos\\theta_{ij} cos\\theta_i^{A} cos\\theta_j^{B}}{\\sqrt{\\sum_{ij} \\lambda_i^2\\gamma_j^2 cos^2\\theta_i^{A}cos^2\\theta_j^{B}} } \\rangle$ \n",
      "\n",
      "Agora precisamos apenas integrar o \u00faltimo termo. De qualquer forma nota-se que o\n",
      "\n",
      "$ RS(A,B) \\leq \\sum_{ij}\\frac{ \\lambda_i\\gamma_j cos^2\\theta_{ij}}{\\sum{ij}\\lambda_i\\gamma_j} $\n",
      "\n",
      "Onde o \u00faltimo termo nada mais \u00e9 do que o PCA similarity factor, a medida de Krzanowski com os pesos dados pelos autovalores.\n",
      "\n",
      "Para isso usaremos:\n",
      "\n",
      "$ (u_i \\cdot \\beta)(s_j \\cdot \\beta) = \\frac{(u_i + s_i \\cdot \\beta)^2 - (u_i \\cdot \\beta)^2  - (s_j \\cdot \\beta)^2}{2} $\n",
      "\n",
      "$ ((u_i + s_i) \\cdot \\beta)^2  = |u_i + s_i|^2 cos^2 \\theta^{A + B}{ij} $\n",
      "\n",
      "$ ((u_i + s_i) \\cdot \\beta)^2 = 4 cos^2 \\frac{\\theta_{ij}}{2} cos^2 \\theta^{A + B}{ij} = 4(\\frac{1 + cos \\theta{ij}}{2}) cos^2 \\theta^{A + B}{ij}$\n",
      "\n",
      "Onde utilizamos no passo intermedi\u00e1rio que o m\u00f3dulo de $u_i + s_j$ \u00e9 igual a diagonal do paralelograma dos vetores $u_i$ e $s_j$. Com isso teremos:\n",
      "\n",
      "$ RS = \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos \\theta_{ij} (u_i \\cdot \\beta)(s_j \\cdot \\beta)}{\\sqrt{\\sum_{lm} \\lambda^2_{l}\\gamma^2_{m}(u_l \\cdot \\beta)^2(s_m \\cdot \\beta)^2}}$\n",
      "\n",
      "$ D(\\beta) = \\sqrt{\\sum_{lm} \\lambda^2_{l}\\gamma^2_{m}(u_l \\cdot \\beta)^2(s_m \\cdot \\beta)^2} $\n",
      "\n",
      "$ RS = \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos \\theta_{ij}  cos^2 \\theta^{A+B}_{ij}}{D(\\beta)} + \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos^2 \\theta_{ij}  cos^2 \\theta^{A+B}_{ij}}{D(\\beta)} - \\frac{1}{2} \\left[  \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos^2 \\theta_{ij}  (u_i \\cdot \\beta)^2}{D(\\beta)} +  \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos^2 \\theta_{ij}  (s_j \\cdot \\beta)^2}{D(\\beta)}\\right]$\n",
      "\n",
      "Mas a primeira e duas \u00faltimas integrais dessa soma ser\u00e3o iguais, j\u00e1 que ao integrarmos em todos vetores de m\u00f3dulo igual a 1 n\u00e3o importar\u00e1 qual vetor escolhermos para o produto interno (seja $u_i$, $s_j$ ou $u_i + s_j$), assim teremos:\n",
      "\n",
      "$ RS = \\sum_{ij} \\int d\\beta \\frac{\\lambda_i \\gamma_j cos^2 \\theta_{ij}  cos^2 \\theta^{A+B}_{ij}}{D(\\beta)}$\n",
      "\n",
      "E usando um argumento an\u00e1logo podemos colocar em evid\u00eancia apenas uma das integrais, j\u00e1 que todas ser\u00e3o iguais:\n",
      "\n",
      "$ RS = \\left[ \\sum_{ij} \\lambda_i \\gamma_j cos^2 \\theta_{ij} \\right] \\int d\\beta \\frac{(u \\cdot \\beta)}{D(\\beta)}$\n",
      "\n",
      "Onde $u$ \u00e9 qualquer vetor."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}